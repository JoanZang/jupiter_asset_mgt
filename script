#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Mar  6 10:54:23 2022

@author: joanzang
"""

import nasdaqdatalink as quandl
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import numpy as np
import yfinance as yf
import os
import warnings
warnings.filterwarnings('ignore')
plt.style.use('fivethirtyeight')
from pylab import rcParams
rcParams['figure.figsize'] = 10, 6
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA, AutoReg
from pmdarima.arima import auto_arima
from pylab import rcParams
from pandas.plotting import autocorrelation_plot
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


#load data from Nasdaq Data
sp_composite = quandl.get("YALE/SPCOMP")

uncertainty= pd.read_excel(r"Copy of US_Policy_Uncertainty_Data.xlsx")

vix_history = pd.read_csv(r"VIX_History.csv")

#data cleaning
sp_composite['month_year'] = pd.to_datetime(sp_composite['Year']).dt.to_period('M')

uncertainty = uncertainty[~uncertainty['Three_Component_Index'].isna()]
uncertainty['month_year'] = pd.to_datetime(uncertainty[['Year', 'Month']].assign(DAY=1)).dt.to_period('M')

vix_history['DATE'] = pd.to_datetime(vix_history['DATE'])
vix_history['month_year'] = pd.to_datetime(vix_history['DATE']).dt.to_period('M')

df_vix_history = vix_history.sort_values('DATE').groupby('month_year').tail(1)

#create column return
sp_composite = sp_composite.reset_index()
sp_composite['return'] = (sp_composite['S&P Composite']- sp_composite['S&P Composite'].shift(1))/(sp_composite['S&P Composite'].shift(1))

#basic statisics for the dataset
summary = sp_composite.describe()

skw = pd.DataFrame(sp_composite.skew()).T
skw.rename(index={0:'Skewness'}, inplace=True)
summary = summary.append(skw)

kur = pd.DataFrame(sp_composite.kurtosis()).T
kur.rename(index={0:'Kurtosis'}, inplace=True)
summary = summary.append(kur)

#boxplot for anomalies for return

box1 = sns.boxplot( y='return', data=sp_composite)
box1.set_xticklabels(box1.get_xticklabels(),rotation=45)
box1.plot()

#anomaly for when return is about 0.5
anomaly = sp_composite.sort_values('return', ascending=False).head(1)
#replace this anomaly with average of the month before and the month after

sp_composite.iloc['1932-08-31']['return']
sp_composite.set_value('1932-08-31', 'return', ((sp_composite['return'].shift(1) + sp_composite['return'].shift(-1))/2))

sp_composite.at[739, 'return'] = ((sp_composite.at[740, 'return'] + sp_composite.at[738,'return'])/2)

return_kur = sp_composite['return'].kurtosis()

sns.distplot(sp_composite['return'], fit=norm, kde=False)

#join vix history with sp composite data
vix_sp = pd.merge(sp_composite, vix_history, how='inner', on='month_year')
#vix_sp['log_close'] = np.log(vix_sp['CLOSE'])

close_return = sns.scatterplot('return','CLOSE', data=vix_sp)
#negative correlation between Close VIX figure and return 

#join uncertainty  with sp composite data
uncertainty_vix = pd.merge(sp_composite, uncertainty, how='inner', on='month_year')
unvertainty_return = sns.scatterplot('return', 'Three_Component_Index', data=uncertainty_vix)


#calcualte VaR for the data
fig = plt.figure()
ax = plt.plot(sp_composite.index, sp_composite['S&P Composite'])

mean = np.mean(sp_composite['return'])
std = np.std(sp_composite['return'])

var_95 = norm.ppf(1-0.95, mean, std)

#calculate expected shortfall
es_95 = sp_composite['return'].lt(var_95).mean()


#calculate drawndown
roll_max = sp_composite['return'].cummax()
daily_drawndown = sp_composite['return']/roll_max-1
max_dail_drawndown = daily_drawndown.cummin()

sp_composite['cumluative_return'] = np.exp(np.log1p(sp_composite['return']).cumsum())

highwatermarks = sp_composite['cumluative_return'].cummax()

drawdowns = (1 + highwatermarks)/(1 + sp_composite['cumluative_return']) - 1

max_drawdown = max(drawdowns.fillna(0))


#question 2
#codes are taken from https://www.kaggle.com/nageshsingh/stock-market-forecasting-arima

df_asml = yf.download('ASML', start='2011-01-01', end='2022-02-28', progress=False)
df_msft = yf.download('MSFT', start='2011-01-01', end='2022-02-28', progress=False)

plt_asml = plt.plot(df_asml['Close'])
plt_msft = plt.plot(df_msft['Close'])

plt_asml_kde = df_asml['Close'].plot(kind='kde')
plt_msft_kde = df_msft['Close'].plot(kind='kde')

def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(12).mean()
    rolstd = timeseries.rolling(12).std()
    #Plot rolling statistics:
    plt.plot(timeseries, color='blue',label='Original')
    plt.plot(rolmean, color='red', label='Rolling Mean')
    plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean and Standard Deviation')
    plt.show(block=False)
    
    print("Results of dickey fuller test")
    adft = adfuller(timeseries,autolag='AIC')
    # output for dft will give us without defining what the values are.
    #hence we manually write what values does it explains using a for loop
    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])
    for key,values in adft[4].items():
        output['critical value (%s)'%key] =  values
    print(output)
    
test_stationarity(df_msft['Close'])

result = seasonal_decompose(df_msft['Close'], model='multiplicative', freq = 30)
fig = plt.figure()  
fig = result.plot()  
fig.set_size_inches(16, 9)

#if not stationary then eliminate trend
#Eliminate trend
rcParams['figure.figsize'] = 10, 6
df_log = np.log(df_msft['Close'])
moving_avg = df_log.rolling(12).mean()
std_dev = df_log.rolling(12).std()
plt.title('Moving Average')
plt.plot(std_dev, color ="black", label = "Standard Deviation")
plt.plot(moving_avg, color="red", label = "Mean")
plt.show()

train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel= 'Dates'
plt.ylabel='Closing Prices'
plt.plot(df_log, 'green', label='Train data')
plt.plot(test_data, 'blue', label='Test data')
plt.legend()
plt.show()

model_autoARIMA = auto_arima(train_data, start_p=0, start_q=0,
                             test='adf',       # use adftest to find optimal 'd'
                             max_p=3, max_q=3, # maximum p and q
                             m=1,              # frequency of series
                             d=None,           # let model determine 'd'
                             seasonal=False,   # No Seasonality
                             start_P=0, 
                             D=0, 
                             trace=True,
                             error_action='ignore',  
                             suppress_warnings=True, 
                             stepwise=True)
print(model_autoARIMA.summary())
model_autoARIMA.plot_diagnostics(figsize=(15,8))
plt.show()

model = ARIMA(train_data, order=(1,1,2))  
fitted = model.fit(disp=-1)  
print(fitted.summary())

fc, se, conf = fitted.forecast(281, alpha=0.05)  # 95% conf


# Make as pandas series
fc_series = pd.Series(fc, index=test_data.index)
lower_series = pd.Series(conf[:, 0], index=test_data.index)
upper_series = pd.Series(conf[:, 1], index=test_data.index)
# Plot
plt.figure(figsize=(10,5), dpi=100)
plt.plot(np.exp(train_data), label='training data')
plt.plot(np.exp(test_data), color = 'blue', label='Actual Stock Price')
plt.plot(fc_series, color = 'orange',label='Predicted Stock Price')
plt.fill_between(lower_series.index, lower_series, upper_series, 
                 color='k', alpha=.10)
plt.legend(loc='upper left', fontsize=8)
plt.show()

mse = mean_squared_error(np.exp(test_data), fc)
print('MSE: '+str(mse))
mae = mean_absolute_error(np.exp(test_data), fc)
print('MAE: '+str(mae))
rmse = math.sqrt(mean_squared_error(np.exp(test_data), fc))
print('RMSE: '+str(rmse))
mape = np.mean(np.abs(fc - np.exp(test_data))/np.abs(np.exp(test_data)))
print('MAPE: '+str(mape))




#this model is not ideal, has very large mean square error,
#mean absolute error and root mean square deviation 

#next model is an autoregression model from https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/
df_asml_close = df_asml[['Close']]
df_msft_close = df_msft[['Close']]

train_data_auto, test_data_auto = df_msft_close[3:int(len(df_asml_close)*0.9)], df_asml_close[int(len(df_asml_close)*0.9):]
autocorrelation_plot(df_msft_close['Close'])
plt.show()

model = AutoReg(train_data_auto, lags=14)
model_fit = model.fit()

# make predictions
predictions = model_fit.predict(start=len(train_data_auto), end=len(train_data_auto)+len(test_data_auto)-1, dynamic=False)

rmse = np.sqrt(mean_squared_error(test_data, predictions))

mse = mean_squared_error(test_data_auto, predictions)
print('MSE: '+str(mse))
mae = mean_absolute_error(test_data_auto, predictions)
print('MAE: '+str(mae))
rmse = math.sqrt(mean_squared_error(test_data_auto, predictions))
print('RMSE: '+str(rmse))
